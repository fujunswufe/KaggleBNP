}
page.rank.centrality <- function(books.mat) {
# PageRank centrality
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node PageRank centrality
bet = page_rank(g)
top = order(bet$vector, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet$vector)*150
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("PageRank")
}
degree.centrality(books.mat)
closeness.centrality(books.mat)
betweenness.centrality(books.mat)
page.rank.centrality(books.mat)##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
# books.idx <- which(colSums(books.mat) > 0)
# books.mat <- books.mat[books.idx, books.idx]
# dim(books.mat)
# replace isbn with book title
## change row name of book.mat, which is a adjacency matrix
## books is the matrix with two columns, respectively isbn and title
change.name <- function(book.mat, books) {
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- names.title
colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
return (book.mat)
}
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
# set.seed(1)
# plot(g, layout = layout.fruchterman.reingold, vertex.size = 4, vertex.label.cex = 0.5)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
## (3)
# Identify the most central nodes in the network based on different centrality measures, degree centrality,
# closeness centrality, betweenness centrality, and PageRank.
# Plot different networks where the nodes are sized based on the centrality measures.
# Highlight the top 5 nodes with the highest centrality measures in each network.
degree.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
# compute node degree centrality
deg <- degree(g)
top <- order(deg, decreasing = T)[1:5]  ## the top-5 nodes with highest degrees
## size node by degree
V(g)$size = abs(deg) * 1
V(g)$color = "gray"
V(g)$label.color = "gray"
V(g)$label.cex = 0.5
E(g)$color = "black"
V(g)[top]$label.color = "black"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
V(g)[top]$color = "Skyblue"
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("degree centrality")
}
closeness.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node closeness centrality
clo = closeness(g)
top = order(clo, decreasing = T)[1:5]
## size node by closeness
V(g)$size = abs(clo)^2 * 1e+05*3
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("closeness")
}
betweenness.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node betweenness centrality
bet = betweenness(g)
top = order(bet, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet)*0.1
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("betweenness")
}
page.rank.centrality <- function(books.mat) {
# PageRank centrality
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node PageRank centrality
bet = page_rank(g)
top = order(bet$vector, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet$vector)*150
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("PageRank")
}
degree.centrality(books.mat)
closeness.centrality(books.mat)
betweenness.centrality(books.mat)
page.rank.centrality(books.mat)
View(books.mat)
##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
# books.idx <- which(colSums(books.mat) > 0)
# books.mat <- books.mat[books.idx, books.idx]
# dim(books.mat)
# replace isbn with book title
## change row name of book.mat, which is a adjacency matrix
## books is the matrix with two columns, respectively isbn and title
change.name <- function(book.mat, books) {
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
return (book.mat)
}
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
# set.seed(1)
# plot(g, layout = layout.fruchterman.reingold, vertex.size = 4, vertex.label.cex = 0.5)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
View(books)
##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
new.row.name <- vector()
for (i in rownames(books.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(books.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(books.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
books.mat <- as.matrix(books.mat)
View(books.mat)
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
# task 1
library(ggplot2)
library(data.table)
library(tm)
library(SnowballC)
library(plyr)
library(lsa)
library(NMF)
# (1)
file.path <- "http://www.yurulin.com/class/fall2015_datamining/data/reuters21578.csv"
reuters <- read.csv(file.path)
# choose the four most popular topics
selected.topics <- sort(table(reuters$topic), decreasing = T)[1:4]
topics <- sort(table(reuters$topic), decreasing = T)
topics <- as.data.frame(topics)
setDT(topics, keep.rownames = TRUE)
colnames(topics) <- c("topic", "count")
qplot(reuters$topic,
geom="histogram",
binwidth = 0.5)
# (2)
# mds.plot <- function() {
selected.topics <- names(selected.topics)
doc.idx = which(reuters$topic %in% selected.topics)
dataset = reuters[doc.idx, ]
# create a corpus
corpus = Corpus(VectorSource(dataset$content))
corpus = tm_map(corpus, content_transformer(tolower)) # to lower case
# corpus = tm_map(corpus, tolower) # to lower case
corpus = tm_map(corpus, removePunctuation) # romove Punctuation
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
corpus = tm_map(corpus, stripWhitespace)
# stemDocument(corpus[[1]]) # there is no package called ‘SnowballC’, add it
inspect(corpus[1:3])
corpus <- tm_map(corpus, stemDocument, language="english")
inspect(corpus[1:3])
# corpus <- tm_map(corpus, PlainTextDocument)
td.mat <- TermDocumentMatrix(corpus)
# td.mat = as.matrix(TermDocumentMatrix(corpus))
freq <- findFreqTerms(td.mat, lowfreq=4) # acquire terms in corpus at least 4 times
td.mat <- as.matrix(td.mat)
td.mat <- td.mat[freq,]
dist.mat = dist(t(td.mat)) # compute distance matrix
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = dataset$topic,
id = row.names(dataset))
ggplot(data, aes(x = x, y = y, color = topic)) + geom_point()
# }
# mds.plot()
# (3)
## tfidf
# tfidf.weighting.mds <- function() {
td.mat.w <- lw_tf(td.mat) * gw_idf(td.mat)  ## tf-idf weighting
k = 4
S = svd(as.matrix(td.mat.w), nu = k, nv = k)
u = S$u
s = S$d
v = S$v
td.mat.svd = S$u %*% diag(S$d[1:k]) %*% t(S$v)
dist.mat = dist(t(td.mat.svd))
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = dataset$topic,
id = row.names(dataset))
ggplot(data, aes(x = x, y = y, color = topic)) + geom_point()
# }
# tfidf.weighting.mds()
# las approximated matrix
# lsa.mds <- function() {
lsa.space = lsa(td.mat.w,dims=4)  ## create LSA space
dist.mat = dist(t(as.textmatrix(lsa.space)))  ## compute distance matrix
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = df$topic, id = row.names(df))
ggplot(data, aes(x = x, y = y, color=topic)) + geom_point()
# }
library(ggplot2)
27555 / 2225213
27555 / 591864
1 - 0.01238308
# Hadley's favourite pie chart
df <- data.frame(
variable = c("reviews with tips", "reviews without tips"),
value = c(0.01238308, 0.9876169)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Pac man")
1 - 0.0465563
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("green", "blue")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("green", "blue")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
# reviews with tips 27555
# reviews 2225213
# tips 591864
df <- data.frame(
variable = c("reviews with tips", "reviews without tips"),
value = c(0.01238308, 0.9876169)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
d <- c("average length of tips in review"= 13.1, "average length of tips not in review" = 10.7)
d <- as.matrix(d)
colnames(d) <- "avg_len"
d <- as.data.frame(d)
qplot(factor(avg_len), data=d, geom="bar", fill=factor(avg_len))
library(ggplot2)
d <- c("average length of tips in review"= 13.1, "average length of tips not in review" = 10.7)
d <- as.matrix(d)
colnames(d) <- "avg_len"
d <- as.data.frame(d)
qplot(factor(avg_len), data=d, geom="bar", fill=factor(avg_len))
View(d)
d[1]
d[1,]
dim(d)
g <- ggplot(d, aes(avg_len))
# Number of cars in each class:
g + geom_bar()
counts <- table(g$avg_len)
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
counts <- table(d$avg_len)
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
mtcar
mtcars
d
dim(d)
dim(mtcars)
counts <- table(mtcars$gear)
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
library(MASS)
avg_len = d$avg_len
barplot(avg_len)
mpg
g <- ggplot(mpg, avg_len)
# Number of cars in each class:
g + geom_bar()
g <- ggplot(d, avg_len)
# Number of cars in each class:
g + geom_bar()
?barplot
library(MASS)
avg_len = d$avg_len
barplot(avg_len, legend.text = c("avg_len for tips in review", "avg_len for tips not in review"))
ggplot(data=d, aes(x=len, y=avg_len, fill=len)) +
geom_bar(colour="black", stat="identity")
dat <- data.frame(
name = factor(c("avg_len for tips in review","avg_len for tips not in review"), levels=c("avg_len for tips in review","avg_len for tips not in review")),
len = c(13.1, 10.7)
)
dat
ggplot(data=dat, aes(x=name, y=len, fill=name)) +
geom_bar(colour="black", stat="identity") +
guides(fill=FALSE)
ggplot(data=dat, aes(x=name, y=len, fill=name)) +
geom_bar(colour="black", stat="identity") + xlab("two kinds of tips") + ylab("length of tips") +
guides(fill=FALSE)
library(readr)
setwd("~/GitHub/KaggleBNP")
train <- read_csv("train.csv")
train <- train[, -1]
active <- as.data.frame(train[1, ])
passive <- as.data.frame(train[1, ])
for (i in 1:114321) {
if (train[i, ]$target == 1) { # complete observation
active <- rbind(active, train[i, ])
} else { # incomplete observation
passive <- rbind(passive, train[i, ])
}
}
mean(is.na(active))
mean(is.na(passive))
